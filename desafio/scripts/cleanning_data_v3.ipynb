{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanning data using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession, dataframe\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/?expand_article=1\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, DateType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from configparser import ConfigParser\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET INIT FILE\n",
    "BASE_CONFIGS = \"/desafio/scripts/config.ini\"\n",
    "config = ConfigParser()\n",
    "config.read(BASE_CONFIGS)\n",
    "\n",
    "# SET VARS\n",
    "DB_EXT = config['HIVE']['DB_EXT'].lower()\n",
    "DB_STG = config['HIVE']['DB_STG'].lower()\n",
    "entidades = config['EDGE']['ENTIDADES'].replace(\"(\", \"\").replace(\")\", \"\").replace('\"', '').split(' ')\n",
    "HDFS_GOLD_DIR = config['HDFS']['HDFS_BASE_DIR'] + \"/gold\"\n",
    "LOG_DIR = config['GERAL']['LOG_DIR']\n",
    "\n",
    "# CONFIG LOGGING\n",
    "logging.basicConfig(filename=f\"{LOG_DIR}/log_desafio.log\",\n",
    "#                     filemode=\"a+\", \n",
    "                    format=\"%(asctime)s %(name)s %(levelname)s %(message)s\",\n",
    "#                     datefmt=\"%Y-%m-%d-%H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logging.info(\"Iniciando aplicação\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe \n",
    "def save_df(_name, _df, _dir):\n",
    "    output = f\"{_dir}/{_name}\"\n",
    "    erase = f\"hdfs dfs -rm {output}/*\"\n",
    "    rename = f\"hdfs dfs -mv {output}/part-*.csv {output}/{_name}.csv\"\n",
    "    \n",
    "    try:\n",
    "        logging.info(erase)\n",
    "        os.system(erase)\n",
    "\n",
    "        logging.info(f\"Transferindo dados: {output}\")\n",
    "        _df.coalesce(1).write\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"header\", True)\\\n",
    "            .option(\"delimiter\", \";\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .save(f\"{output}/\")\n",
    "        \n",
    "        logging.info(rename)\n",
    "        os.system(rename)\n",
    "        \n",
    "        logging.info(f\"{_name} persistida com sucesso em {output}\\n\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro: {e}\")\n",
    "        pass\n",
    "\n",
    "# Process dataframes\n",
    "def process_dataframe_rdd(_dataframe:dataframe.DataFrame, _datatypes:dict) -> dataframe.DataFrame:\n",
    "    clean_string_udf = udf(lambda x: clean_empty(x), StringType())\n",
    "    clean_zero_udf = udf(lambda x : \"0.0\" if x == \"\" else x)\n",
    "    cast_float_udf = udf(lambda x: float(str(x).strip().replace('.', '').replace(',', '.')))\n",
    "    mapping_types  = {\n",
    "        'float': DoubleType(),\n",
    "        'object': StringType(), \n",
    "        'datetime64[ns]': DateType()\n",
    "    }\n",
    "    for field, _type in _datatypes.items():\n",
    "        if _type == 'float':\n",
    "            _dataframe = _dataframe.withColumn(field, clean_zero_udf(col(field)))\n",
    "            _dataframe = _dataframe.withColumn(field, cast_float_udf(col(field)))\n",
    "            \n",
    "        if _type == 'object':\n",
    "            _dataframe = _dataframe.withColumn(field, clean_string_udf(col(field)))\n",
    "        \n",
    "        _dataframe = _dataframe.withColumn(field, col(field).cast(mapping_types[_type]))        \n",
    "    \n",
    "    return _dataframe\n",
    "\n",
    "\n",
    "# Limpeza de campos string\n",
    "def clean_empty(_txt:str) -> str:\n",
    "    if _txt == \"\":\n",
    "        return \"Não informado\"\n",
    "    \n",
    "    rs = re.search(\"\\s{2,}\", _txt)\n",
    "    \n",
    "    if rs != None:\n",
    "    \n",
    "        if rs.span()[0] == 0:\n",
    "            return \"Não informado\"\n",
    "        _txt = _txt.replace(rs.group(), ' ')\n",
    "\n",
    "    return _txt\n",
    "\n",
    "assert clean_empty(\"\") == \"Não informado\", \"Precisa retornar 'Não informado'\"\n",
    "assert clean_empty(\"    \") == \"Não informado\"\n",
    "assert clean_empty(\"New York\") == \"New York\"\n",
    "assert clean_empty(\"New  York\") == \"New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configruação dos tipos das tabelas\n",
    "_types = {\n",
    "# regiao\n",
    "'regiao': {'region_code': 'object', \n",
    "                    'region_name': 'object', \n",
    "                    'dt_foto': 'datetime64[ns]'}, \n",
    "#divisao\n",
    "'divisao': {'division': 'object', \n",
    "    'division_name': 'object', \n",
    "    'dt_foto': 'datetime64[ns]'},\n",
    "# vendas\n",
    "'vendas': {'actual_delivery_date': 'datetime64[ns]',\n",
    " 'customerkey': 'object',\n",
    " 'datekey': 'datetime64[ns]',\n",
    " 'discount_amount': 'float',\n",
    " 'invoice_date': 'datetime64[ns]',\n",
    " 'invoice_number': 'object',\n",
    " 'item_class': 'object',\n",
    " 'item_number': 'object',\n",
    " 'item': 'object',\n",
    " 'line_number': 'object',\n",
    " 'list_price': 'float',\n",
    " 'order_number': 'object',\n",
    " 'promised_delivery_date': 'datetime64[ns]',\n",
    " 'sales_amount': 'float',\n",
    " 'sales_amount_based_on_list_price': 'float',\n",
    " 'sales_cost_amount': 'float',\n",
    " 'sales_margin_amount': 'float',\n",
    " 'sales_price': 'float',\n",
    " 'sales_quantity': 'float',\n",
    " 'sales_rep': 'float',\n",
    " 'u_m': 'object',\n",
    " 'dt_foto': 'datetime64[ns]'}, \n",
    "# endereco\n",
    "'endereco': {'address_number': 'object', \n",
    " 'city': 'object', \n",
    " 'country': 'object', \n",
    " 'customer_address_1': 'object',\n",
    "'customer_address_2': 'object', \n",
    " 'customer_address_3': 'object', \n",
    " 'customer_address_4': 'object',\n",
    "'state': 'object', \n",
    " 'zip_code': 'object', \n",
    " 'dt_foto': 'datetime64[ns]'}, \n",
    "# clientes\n",
    "'clientes': {'address_number': 'object',\n",
    " 'business_family': 'object',\n",
    " 'business_unit': 'object',\n",
    " 'customer': 'object',\n",
    " 'customerkey': 'object',\n",
    " 'customer_type': 'object',\n",
    " 'division': 'object',\n",
    " 'line_of_business': 'object',\n",
    " 'phone': 'object',\n",
    " 'region_code': 'object',\n",
    " 'regional_sales_mgr': 'object',\n",
    " 'search_type': 'object',\n",
    " 'dt_foto': 'datetime64[ns]'}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-apply-function-to-column/\n",
    "logging.info(\"Iniciando persistência das entidades para HDFS\")\n",
    "for entidade in entidades:\n",
    "    data = spark.sql(f\"select * from {DB_STG}.tbl_{entidade}\")\n",
    "    data = process_dataframe_rdd(data, _types[entidade])\n",
    "    logging.info(f\"Exportando {HDFS_GOLD_DIR}/{entidade}/{entidade}.csv\")\n",
    "    save_df(_df=data, _dir=HDFS_GOLD_DIR, _name=entidade)\n",
    "    logging.info(\"Exportação de tabelas realizada com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-update-a-column-with-value/?expand_article=1\n",
    "# data.createOrReplaceTempView(\"VENDAS_TEMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_df = spark.sql(\"select * from VENDAS_TEMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd() + \"/parquet/\"\n",
    "# create_df.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endereco = spark.sql(\"select * from desafio_stg_db.tbl_endereco limit 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endereco.columns\n",
    "# endereco.select('city').collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanUDF = udf(lambda x: clean_empty(x), StringType())\n",
    "# endereco = endereco.withColumn('city', cleanUDF(col('city')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endereco.select('city').collect()\n",
    "# endereco.withColumn('city', col('city').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endereco.select('city').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
